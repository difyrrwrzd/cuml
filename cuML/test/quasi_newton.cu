//#include "glm/lr_lasso.h"
#include <gtest/gtest.h>
#include <linalg/cublas_wrappers.h>
#include <linalg/cusolver_wrappers.h>

#include "test_utils.h"
#include <cuda_utils.h>
//#include "ml_utils.h"

#include <math.h>
#include <stdlib.h>

#include <glm/glm_batch_gradient.h>
#include <glm/glm_logistic.h>
#include <glm/glm_softmax.h>
#include <glm/glm_linear.h>
#include <glm/gradient_descent.h>
#include <glm/lbfgs.h>
#include <glm/qn_c.h>
#include <linalg/transpose.h>

using namespace ML;
using namespace ML::GLM;
template <typename T, typename LossFunction>
int fit_dispatch(T *X, T *y, int N, int D, bool has_bias, T l1, T l2,
                 int max_iter, T grad_tol, T value_rel_tol,
                 int linesearch_max_iter, int lbfgs_memory, int verbosity,
                 T *w0, // initial value and result
                 T *fx, int *num_iters);

template <typename T, typename LossFunction, STORAGE_ORDER Storage = COL_MAJOR>
int qn_fit(LossFunction *loss, T *Xptr, T *yptr, T *zptr, int N, bool has_bias,
           T l1, T l2, int max_iter, T grad_tol, T value_rel_tol,
           int linesearch_max_iter, int lbfgs_memory, int verbosity,
           T *w0, // initial value and result
           T *fx, int *num_iters);

namespace ML {
namespace GLM {

using namespace MLCommon;

struct QuasiNewtonTest : ::testing::Test {
  cublasHandle_t cublas;
  void SetUp() { cublasCreate(&cublas); }

  void TearDown() {}
};

struct InputSpec {
  int n_row;
  int n_col;
  bool fit_intercept;
};

template <class T> struct DevUpload {
  SimpleMat<T> devX;
  SimpleVec<T> devY;
  DevUpload(const InputSpec &inSpec, const T *x, const T *y,
            cublasHandle_t &cublas)
      : devX(inSpec.n_row, inSpec.n_col), devY(inSpec.n_row) {

    SimpleMat<T> devXtmp(inSpec.n_row, inSpec.n_col);

    updateDevice(devX.data, x, inSpec.n_row * inSpec.n_col);
    updateDevice(devY.data, y, inSpec.n_row);
  }
};

template <typename T> T run(InputSpec &in, DevUpload<T> &devUpload, T l1) {
  int N = in.n_row, D = in.n_col;
  bool has_bias = in.fit_intercept;
  T l2 = 0.0;
  SimpleVec<T> w(D + has_bias);
  int max_iter = 100;
  T grad_tol = 1e-5;
  T value_rel_tol = 1e-5;
  int linesearch_max_iter = 40;
  int lbfgs_memory = 2;
  int verbosity = 1;
  int num_iters = 0;
  T lassoObjective = 0;
  fit_dispatch<T, SquaredLoss<T, ROW_MAJOR>>(
      devUpload.devX.data, devUpload.devY.data, N, D, has_bias, l1, l2,
      max_iter, grad_tol, value_rel_tol, linesearch_max_iter, lbfgs_memory,
      verbosity,
      w.data, // initial value and result
      &lassoObjective, &num_iters);

  return lassoObjective;
}

TEST_F(QuasiNewtonTest, QN_Lasso_Test_DesignMatrix_5_2_and_alpha_eq_0) {
  {
    // =====================AUTOGENERATED CODE
    // START===============================
    //  CODE IS GENERATED VIA 'Test_DesignMatrix_5_2_and_alpha_eq_0.py' SCRIPT
    //  Features Marix (Design Matrix)
    double X[5][2] = {{0.474571, 0.657473},
                      {0.142600, 0.010860},
                      {0.274048, 0.810348},
                      {0.601457, 0.558190},
                      {0.145303, 0.440055}};

    // Targets (Labels)

    double Y[5] = {6.663885, 0.500833, 7.400750, 6.210172, 2.916327};
    // Objective value obtained during using this model for fitting <class
    // 'sklearn.linear_model.coordinate_descent.Lasso'>
    double kObjectiveValueFromSkLearn = 0.094153;
    double kAlphaSkLearn = 0.000000;
    // =====================AUTOGENERATED CODE
    // END=================================

    InputSpec in;
    in.n_row = 5;
    in.n_col = 2;
    in.fit_intercept = false;

    DevUpload<double> devUpload(in, &X[0][0], &Y[0], cublas);

    double lassoObjective = run(in, devUpload, kAlphaSkLearn);

    const double kObjectiveTolerance = 1e-5;
    EXPECT_TRUE(fabs(lassoObjective - kObjectiveValueFromSkLearn) <
                kObjectiveTolerance)
        << "Check that result is the same as for SkLearn";
  }
}

TEST_F(QuasiNewtonTest, QN_Lasso_Test_DesignMatrix_4_7_and_alpha_eq_2) {
  {
    // =====================AUTOGENERATED CODE
    // START===============================
    //  CODE IS GENERATED VIA 'Test_DesignMatrix_4_7_and_alpha_eq_2.py' SCRIPT
    //  Features Marix (Design Matrix)
    double X[4][7] = {
        {0.474571, 0.657473, 0.666410, 0.142600, 0.010860, 0.374754, 0.274048},
        {0.690593, 0.601457, 0.558190, 0.661321, 0.145303, 0.440055, 0.162267},
        {0.058824, 0.818820, 0.074610, 0.686946, 0.337000, 0.404614, 0.842403},
        {0.060785, 0.915034, 0.508926, 0.090978, 0.987134, 0.946713, 0.112528}};

    // Targets (Labels)

    double Y[4] = {7.792896, 6.285808, 6.629604, 12.927304};
    // Objective value obtained during using this model for fitting <class
    // 'sklearn.linear_model.coordinate_descent.Lasso'>
    double kObjectiveValueFromSkLearn = 21.000897;
    double kAlphaSkLearn = 2.000000;
    // =====================AUTOGENERATED CODE
    // END=================================

    InputSpec in;
    in.n_row = 4;
    in.n_col = 7;
    in.fit_intercept = false;

    DevUpload<double> devUpload(in, &X[0][0], &Y[0], cublas);

    double lassoObjective = run(in, devUpload, kAlphaSkLearn);

    const double kObjectiveTolerance = 1e-5;
    EXPECT_TRUE(fabs(lassoObjective - kObjectiveValueFromSkLearn) <
                kObjectiveTolerance)
        << "Check that result is the same as for SkLearn";
  }
}

TEST_F(QuasiNewtonTest, QN_Lasso_Test_DesignMatrix_10_2_and_alpha_eq_half) {
  {
    // =====================AUTOGENERATED CODE
    // START===============================
    //  CODE IS GENERATED VIA 'Test_DesignMatrix_10_2_and_alpha_eq_half.py'
    //  SCRIPT Features Marix (Design Matrix)
    double X[10][2] = {{0.474571, 0.657473}, {0.142600, 0.010860},
                       {0.274048, 0.810348}, {0.601457, 0.558190},
                       {0.145303, 0.440055}, {0.905973, 0.058824},
                       {0.074610, 0.686946}, {0.404614, 0.842403},
                       {0.060785, 0.915034}, {0.090978, 0.987134}};

    // Targets (Labels)

    double Y[10] = {8.100475,  1.791319, 6.249202, 9.296194, 2.257921,
                    12.022210, 2.586373, 5.814886, 3.513386, 5.354221};
    // Objective value obtained during using this model for fitting <class
    // 'sklearn.linear_model.coordinate_descent.Lasso'>
    double kObjectiveValueFromSkLearn = 7.303088;
    double kAlphaSkLearn = 0.500000;
    // =====================AUTOGENERATED CODE
    // END=================================

    InputSpec in;
    in.n_row = 10;
    in.n_col = 2;
    in.fit_intercept = false;

    DevUpload<double> devUpload(in, &X[0][0], &Y[0], cublas);

    double lassoObjective = run(in, devUpload, kAlphaSkLearn);

    const double kObjectiveTolerance = 1e-5;
    EXPECT_TRUE(fabs(lassoObjective - kObjectiveValueFromSkLearn) <
                kObjectiveTolerance)
        << "Check that result is the same as for SkLearn";
  }
}

template <typename T>
T run_logistic(DevUpload<T> &devUpload, InputSpec &in, T l1, T l2, T *w,
               SimpleVec<T> &z) {

  int max_iter = 100;
  T grad_tol = 1e-8;
  T value_rel_tol = 1e-5;
  int linesearch_max_iter = 50;
  int lbfgs_memory = 5;
  int verbosity = 0;
  int num_iters = 0;

  T fx;
  LogisticLoss1<T> loss(in.n_col, in.fit_intercept);
  SimpleVec<T> w0(w, loss.n_param);

  qn_fit<T, LogisticLoss1<T>, ROW_MAJOR>(
      &loss, devUpload.devX.data, devUpload.devY.data, z.data, in.n_row,
      loss.fit_intercept, l1, l2, max_iter, grad_tol, value_rel_tol,
      linesearch_max_iter, lbfgs_memory, verbosity, w0.data, &fx, &num_iters);

  return fx;
}

TEST_F(QuasiNewtonTest, binary_logistic_vs_sklearn) {
    // Test case generated in python and solved with sklearn 
  double X[10][2] = {{-0.2047076594847130, 0.4789433380575482},
                     {-0.5194387150567381, -0.5557303043474900},
                     {1.9657805725027142, 1.3934058329729904},
                     {0.0929078767437177, 0.2817461528302025},
                     {0.7690225676118387, 1.2464347363862822},
                     {1.0071893575830049, -1.2962211091122635},
                     {0.2749916334321240, 0.2289128789353159},
                     {1.3529168351654497, 0.8864293405915888},
                     {-2.0016373096603974, -0.3718425371402544},
                     {1.6690253095248706, -0.4385697358355719}};
  double y[10] = {1, 1, 1, 0, 1, 0, 1, 0, 1, 0};

  InputSpec in;
  in.n_row = 10;
  in.n_col = 2;
  double alpha = 0.01;

  SimpleVec<double> w0(in.n_col + 1);
  SimpleVec<double> z(in.n_row);

  DevUpload<double> devUpload(in, &X[0][0], &y[0], cublas);
  double l1, l2, fx;

  in.fit_intercept = true;
  double w_l1_b[2] = {-1.6899370396155091, 1.9021577534928300};
  double b_l1_b = 0.8057670813749118;
  double obj_l1_b = 0.44295941481024703;

  fx = run_logistic(devUpload, in, alpha, 0.0, w0.data, z);

  w0.print();
  printf("Ref=%f, %f\n", obj_l1_b, fx);

  in.fit_intercept = true;
  double w_l2_b[2] = {-1.5339880402781370, 1.6788639581350926};
  double b_l2_b = 0.806087868102401;
  double obj_l2_b = 0.4378085369889721;

  fx = run_logistic(devUpload, in, 0.0, alpha, w0.data, z);
  w0.print();
  printf("Ref=%f, %f\n", obj_l2_b, fx);

  in.fit_intercept = false;
  double w_l1_no_b[2] = {-1.6215035298864591, 2.3650868394981086};
  double obj_l1_no_b = 0.4769896009200278;

  fx = run_logistic(devUpload, in, alpha, 0.0, w0.data, z);
  w0.print();
  printf("Ref=%f, %f\n", obj_l1_no_b, fx);

  in.fit_intercept = false;
  double w_l2_no_b[2] = {-1.3931049893764620, 2.0140103094119621};
  double obj_l2_no_b = 0.47502098062114273;

  fx = run_logistic(devUpload, in, 0.0, alpha, w0.data, z);
  w0.print();
  printf("Ref=%f, %f\n", obj_l2_no_b, fx);


  int max_iter = 100;
  double   grad_tol = 1e-8;
  double value_rel_tol = 1e-5;
  int linesearch_max_iter = 50;
  int lbfgs_memory = 5;
  int verbosity = 0;
  int num_iters = 0;
  LBFGSParam<double> opt_param;
  opt_param.epsilon = grad_tol;
  opt_param.delta = value_rel_tol;
  opt_param.max_iterations = max_iter;
  opt_param.m = lbfgs_memory;
  opt_param.max_linesearch = linesearch_max_iter;
 

  int C = 4, N = 10, D=2;
  SimpleMat<double> Z(C, N);
  SimpleVec<double> wsm(C * D);
  wsm.fill(0);
  SimpleVec<double> gsm(C * D);
  Softmax<double> test(D,C, false);

  GLMWithData<double, Softmax<double>, ROW_MAJOR> lossWith(
      &test, devUpload.devX.data, devUpload.devY.data, Z.data, N);
  double sm = lossWith(wsm, gsm);

    qn_minimize(wsm, &fx, &num_iters, lossWith, 0.0, opt_param, 1);
  wsm.print();
}

} // namespace GLM
} // end namespace ML
